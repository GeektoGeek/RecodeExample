install.packages("h2o")
library(h2o)
install.packages("data.table")
q()
install.packages("logistf")
library(logistf)
data(sex2)
View(sex2)
summary(sex2)
df<- sex2
summary(df$case)
library(data.table)
install.packages("data.table")
library(data.table)
library(data.table)
data.table(summary$case)
data.table(summary(df$case))
df1<- data.table(summary(df$case))
prop.table(table(df$case))
install.packages("caret")
library(caret)
set.seed(3033)
index <- createDataPartition(y = df$case, p= 0.7, list = FALSE)
training <- train[index,]
testing <- train[-index,]
library(caret)
set.seed(3033)
index <- createDataPartition(y = df$case, p= 0.7, list = FALSE)
training <- df[index,]
testing <- df[-index,]
fit.glm<- glm(case~age+oc+vic+vicl+vis+dia,data=training,family=binomial())
install.packages("pscl")
library(pscl)
pR2(fit.glm)
library(pscl)
pR2(fit.glm)
pR2(fit.glm)
install.packages("pscl")
library(pscl)
library(pscl)
pR2(fit.glm)
pred<- predict(fit.glm,newdata=testing, type='response')
pred<- predict(fit.glm,newdata=testing, type='class')
testing<- cbind(testing,pred)
View(testing)
testing$pred <- ifelse(pred > 0.5, 1,0)
summary(fit.glm)
Accuracy<- 1- (mean(testing$case!= testing$pred))
Accuracy
fit_penalizedLogistic<-logistf(case ~ age+oc+vic+vicl+vis+dia, data=training)
install.packages("logistf")
library(logistf)
fit_penalizedLogistic<-logistf(case ~ age+oc+vic+vicl+vis+dia, data=training)
summary(fit_penalizedLogistic)
pR2(fit_penalizedLogistic)
pred_logispenal<- predict(fit_penalizedLogistic,newdata=testing, type='response')
pred_logispenal<- predict(fit, newdata = testing[1:5, ], type = 'response')
pred_logispenal<- predict(fit_penalizedLogistic, newdata = testing[1:5, ], type = 'response')
install.packages("brglm")
library(brglm)
library(brglm)
pred_logispenal<- predict(fit_penalizedLogistic, newdata = testing[1:5, ], type = 'response')
fit_penalizedLogistic<-brglm(case ~ age+oc+vic+vicl+vis+dia, data=training)
Pred1<- predict(fit_penalizedLogistic, newdata = testing[1:5, ], type = "response")
testing<- cbind(testing,pred1)
testing<- cbind(testing,Pred1)
Pred1<- predict(fit_penalizedLogistic, newdata = testing[1:71, ], type = "response")
testing<- cbind(testing,Pred1)
testing$Pred1 <- ifelse(Pred1 > 0.5, 1,0)
Accuracy1<- 1- (mean(testing$case!= testing$Pred1))
#### How to boot strap a specific mean or median from multiple samples..
meanf<- function(x,i){mean(x[i])}
bootmean<- boot(data=mtcars$mpg, meanf, R=1000)
install.packages("boot")
library(boot)
bootmean<- boot(data=mtcars$mpg, meanf, R=1000)
bootmean
meanf<- function(x,i){mean(x[i])}
bootmean<- boot(data=mtcars$mpg, meanf, R=1000)
bootmean
medianf<- function(x,i){median(x[i])}
bootmedian<- boot(data=mtcars$mpg, medianf,R=1000)
bootmedian
bootCI <- boot.ci(median.boot, conf = .90, type="bca")
bootCI <- boot.ci(bootmedian, conf = .90, type="bca")
bootCI
### Calulate mean
x<- norm(1000)
x<- rnorm(10000)
boot.mean<- function(x,i){x[i]}
voot<- boot(data=mtcars, boot.mean, 10000)
voot<- boot(data=mtcars$mpg, boot.mean, R=10000)
library(boot)
x<- rnorm(10000)
boot.mean<- function(x,i){x[i]}
voot<- boot(data=mtcars$mpg, boot.mean, R=10000)
boot.median<- function(x,i){median=x[i]}
voot1<- boot(data=mtcars$mpg, boot.median, R=10000)
SE.boot <- sd(as.vector(boot.median$t))
SE.boot <- sd(as.vector(boot.median$t))
SE.boot     <- sd(as.vector(boot.median$t))
SE.boot     <- sd(voot1)
SE.boot     <- sd(as.vector(boot.median))
boot.mean
voot
voot1
custom.boot <- function(times, data=df) {
boots <- rep(NA, times)
for (i in 1:times) {
boots[i] <- sd(sample(data, length(data), replace=TRUE))/sqrt(length(data))
}
boots
}
custom.boot()
custom.boot
boot.median<- function(x,i){median=x[i]}
voot1<- boot(mtcars$mpg, boot.median, R=1000)
voot1
set.seed(1000)
x<- rnorm(10000)
meanfunc<- function(x,i){mean(x[i])}
bootmean<- boot(data=mtcars$mpg, meanfunc, R=1000)
bootmean
library(boot)
f.med       <- function(Y,i){median(Y[i])}
median.boot <- boot(data=mtcars$mpg, f.med, R = 10000)
## Calculating the Standard Error
SE.boot     <- sd(as.vector(median.boot$t))
SE.boot
### Confidence Interval
median.int.90 <- mean(median.boot$t) + qt( c(0.05, 0.95), length(x) - 1) * SE.boot
median.int.90
SE.boot     <- sd(as.vector(voot1$t))
SE.boot
median.int.90 <- mean(voot1$t) + qt( c(0.05, 0.95), length(x) - 1) * SE.boot
median.int.90
setwd("~/CodeForAllProjects/R/KnowData_Project/R&DWithPre-Burn-IN/PoC_PreburnIN/With_Pass_Fail_Label")
library(data.table)
library(plyr)
library(dplyr)
library(gdata)
library(corrgram)
library(glmnet)
library(caret)
library(ISLR)
library(psych)
library(keras)
library(lime)
library(tidyquant)
library(rsample)
library(recipes)
library(yardstick)
library(corrr)
library(psych)
library(keras)
library(lime)
library(tidyquant)
library(rsample)
Data<- read.csv("FinalData1.csv", sep=",")
View(Data)
View(Data)
ds2<- Data[!(rowSums(is.na(Data))),]
prop.table(table(ds2$Pass1_Fail0))
library(plyr)
library(dplyr)
library(gdata)
library(corrgram)
library(glmnet)
library(caret)
library(ISLR)
library(h2o)
View(ds2)
ds3<- ds2 %>% select(-OSC, - Date, -SetCurr,-CyclesComp,-ModelockFaults,-MaxMLTime,-CountML_t_gt_tar,-CountMl_t_gt_spec,-AvgPower,-X3Sigma,-Peak_Peak, -PowerDrift)
library(caret)
library(h2o)
ds4<- ds3
ds4$Pass1_Fail0<- as.factor(ds4$Pass1_Fail0)
a <- createDataPartition(ds4$Pass1_Fail0, p = 0.8, list=FALSE)
training <- ds4[a,]
testing <- ds4[-a,]
training$Pass1_Fail0
table(training$Pass1_Fail0)
table(testing$Pass1_Fail0)
a1 <- createDataPartition(training$Pass1_Fail0, p = 0.8, list=FALSE)
train <- training[a1,]
valid <- training[-a1,]
test <- testing
library(h2o)
h2o.init(ip="localhost", port=54321, max_mem_size="1250m")
train_h2o <- as.h2o(train)
valid_h2o <- as.h2o(valid)
test_h2o <- as.h2o(test)
y <- "Pass1_Fail0"
x <- setdiff(names(train_h2o), c(y))
set.seed(1000)
automl_models_h2o <- h2o.automl(
x = x,
y = y,
training_frame    = train_h2o,
validation_frame  = valid_h2o,
max_runtime_secs  = 45
)
automl_models_h2o@leader
pred_h2o <- h2o.predict(automl_models_h2o@leader, newdata = valid_h2o)
pred<- as.data.frame(pred_h2o)
valid_label<- cbind(valid,pred)
Acc_AutoML<- 1- (mean(valid_label$Pass1_Fail0!= valid_label$predict))
automl_models_h2o@leader
pred_h2o1 <- h2o.predict(automl_models_h2o@leader, newdata = test_h2o)
pred1<- as.data.frame(pred_h2o)
test_label<- cbind(test,pred1)
automl_models_h2o@leader
pred_h2o1 <- h2o.predict(automl_models_h2o@leader, newdata = test_h2o)
pred1<- as.data.frame(pred_h2o1)
test_label<- cbind(test,pred1)
pred1<- as.data.frame(pred_h2o)
test_label1<- cbind(test,pred1)
pred1<- as.data.frame(pred_h2o1)
test_label1<- cbind(test,pred1)
View(test_label1)
Acc_AutoML1<- 1- (mean(test_label1$Pass1_Fail0!= test_label1$predict))
Acc_AutoML1
### Confusion Metrix
table(valid_label$Pass1_Fail0, valid_label$predict)
### Confusion Metrix
table(test_label1$Pass1_Fail0, test_label1$predict)
perf <- h2o.performance(automl_models_h2o@leader, newdata = test_h2o)
h2o.plot(perf)
h2o.varimp_plot(automl_models_h2o@leader)
str(ds4)
#### Some things are loadeed as Factor
ds5 <- factor(c(28, 69, 36)); as.numeric(ds4)
ds5<- as.data.frame(lapply(ds4, function(x) as.numeric(levels(x))[x]))
function(x) as.numeric(levels(x))[x]
ds5<- lapply(ds4, as.numeric)
ds5 = as.matrix(as.data.frame(lapply(ds4, as.numeric)))
View(ds5)
View(ds4)
View(ds4)
ds4$PER_NoEp <-as.numeric(as.character(ds4$PER_NoEp))
ds4<- ds3
####
DataN<- read.csv("FinalData1modi.csv", sep=",")
str(DataN)
DataN<- read.csv("FinalData1modi.csv", sep=",")
str(DataN)
DataN1<- read.csv("FinalData1modi2.csv", sep=",")
str(DataN1)
str(DataN1$Optical_PW_10C)
str(DataN1$Ope_Curr_CuredFer)
str(DataN1$PER_NoEp)
View(DataN1)
str(DataN1)
DataN1<- read.csv("FinalData1modi2.csv", sep=",")
str(DataN1)
save.image("~/CodeForAllProjects/R/KnowData_Project/R&DWithPre-Burn-IN/PoC_PreburnIN/With_Pass_Fail_Label/Code-5-18CleanData.RData")
savehistory("~/CodeForAllProjects/R/KnowData_Project/R&DWithPre-Burn-IN/PoC_PreburnIN/With_Pass_Fail_Label/Code-5-18CleanData.RData")
###
summary(DataN1)
DDD<- DataN1[!(rowSums(is.na(DataN1))),]
summary(DDD)
View(DDD)
DDD1 %>% select(-OSC, - Date, -SetCurr,-CyclesComp,-ModelockFaults,-MaxMLTime,-CountML_t_gt_tar,-CountMl_t_gt_spec,-AvgPower,-X3Sigma,-Peak_Peak, -PowerDrift)
DDD1<- DDD  %>% select(-OSC, - Date, -SetCurr,-CyclesComp,-ModelockFaults,-MaxMLTime,-CountML_t_gt_tar,-CountMl_t_gt_spec,-AvgPower,-X3Sigma,-Peak_Peak, -PowerDrift)
View(DDD1)
str(DDD1)
DDD1$Pass1_Fail0 <- as.factor(DDD1$Pass1_Fail0)
View(DDD1)
library(caret)
library(h2o)
table(DDD1$Pass1_Fail0)
prop.table(table(DDD1$Pass1_Fail0))
b <- createDataPartition(DDD1$Pass1_Fail0, p = 0.8, list=FALSE)
trainingDDD <- DDD1[b,]
testingDDD <- DDD1[-b,]
prop.table(table(trainingDDD$Pass1_Fail0))
prop.table(table(testingDDD$Pass1_Fail0))
b1 <- createDataPartition(trainingDDD$Pass1_Fail0, p = 0.8, list=FALSE)
trainDDD <- trainingDDD[b1,]
validDDD <- trainingDDD[-b1,]
testDDD <- testingDDD
library(h2o)
h2o.init(ip="localhost", port=54321, max_mem_size="1250m")
trainDDD_h2o <- as.h2o(trainDDD)
validDDD_h2o <- as.h2o(validDDD)
testDDD_h2o <- as.h2o(testDDD)
View(trainingDDD)
y <- "Pass1_Fail0"
x <- setdiff(names(trainDDD_h2o), c(y))
set.seed(1008)
automl_models_h2oDDD <- h2o.automl(
x = x,
y = y,
training_frame    = trainDDD_h2o,
validation_frame  = validDDD_h2o,
max_runtime_secs  = 45
)
automl_models_h2oDDD@leader
pred_validDDDh2o <- h2o.predict(automl_models_h2oDDD@leader, newdata = validDDD_h2o)
pred_validDDD<- as.data.frame(pred_validDDDh2o)
validDDD_label<- cbind(validDDD,pred_validDDD)
Acc_AutoMLValidDD<- 1- (mean(validDDD_label$Pass1_Fail0!= validDDD_label$predict))
pred_testDDDh2o <- h2o.predict(automl_models_h2oDDD@leader, newdata = testDDD_h2o)
pred_testDDD<- as.data.frame(pred_testDDDh2o)
testDDD_label<- cbind(testDDD,pred_testDDD)
Acc_AutoMLtestDD<- 1- (mean(testDDD_label$Pass1_Fail0!= testDDD_label$predict))
table(validDDD_label$Pass1_Fail0, validDDD_label$predict)
table(testDDD_label$Pass1_Fail0, testDDD_label$predict)
perf <- h2o.performance(automl_models_h2oDDD@leader, newdata = testDDD_h2o)
h2o.plot(perf)
plot(perf)
h2o.varimp_plot(automl_models_h2oDDD@leader)
savehistory("~/CodeForAllProjects/R/KnowData_Project/R&DWithPre-Burn-IN/PoC_PreburnIN/With_Pass_Fail_Label/Code-5-18CleanDataFinal.Rhistory")
perf1 <- h2o.performance(automl_models_h2oDDD@leader, newdata = testDDD_h2o)
plot(h2o.performance(perf1))
plot(perf1)
perf1
savehistory("~/CodeForAllProjects/R/KnowData_Project/R&DWithPre-Burn-IN/PoC_PreburnIN/With_Pass_Fail_Label/Code-5-18CleanDataFinal.Rhistory")
